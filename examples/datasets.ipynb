{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvideo Dataset demo\n",
    "\n",
    "This notebook demonstrates the `VideoDataset` classes \n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Set up](#Set-up)\n",
    "  1. [Imports](#Imports)\n",
    "  2. [Downloading media](#Downloading-media)\n",
    "2. [The `VideoDataset` classes](#The-VideoDataset-classes)\n",
    "  1. [ImageFolderVideoDataset](#ImageFolderVideoDataset)\n",
    "  2. [VideoFolderDataset](#VideoFolderDataset)\n",
    "  3. [GulpVideoDataset](#GulpVideoDataset)\n",
    "3. [Labels](#Labels)\n",
    "4. [Sampling frames](#Frame-sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the library path to sys.path so we can import torchvideo\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvideo.transforms import *\n",
    "from torchvideo.datasets import *\n",
    "from torchvideo.samplers import *\n",
    "from torchvideo.datasets.vis import show_video\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download a test video and prepare the some toy datasets. We'll reuse the media used to test `torchvideo`. The `gen_test_media` script will download a short clip of [Big Buck Bunny](https://peach.blender.org/) and create datasets suitable for use with all `VideoDataset` classes:\n",
    "\n",
    "- An [`ImageFolderVideoDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#imagefoldervideodataset) where each example is comprised of a set of frames stored as images on disk.\n",
    "- A [`VideoFolderDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#videofolderdataset) where each example is stored as a video file.\n",
    "- A [`GulpVideoDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#gulpvideodataset) where frames are stored in a simple binary format of concatenated JPEGs (see the [GulpIO](https://github.com/TwentyBN/GulpIO) README for more info on this format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the test media\n",
    "if [[ -f ../tests/data/media/big_buck_bunny_360p_5mb.mp4 ]]; then\n",
    "  cd ../tests/data/media \n",
    "  ./gen_test_media.sh > /dev/null 2>&1\n",
    "  cd -\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can play around with `torchvideo`'s dataset classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VideoDataset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can play around with `torchvideo`'s dataset classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageFolderVideoDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset class you're looking for if you videos have been dumped into individual frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should the data be layed out on disk?\n",
    "The root dataset folder should contain subdirectories for each example. Each subdirectory should contain numbered images corresponding to each frame from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../tests/data/media\n",
    "echo \"Top level folder contents: \"\n",
    "ls -l video_image_folder \n",
    "echo\n",
    "\n",
    "echo \"Example folder contents: \"\n",
    "ls -l video_image_folder/video0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderVideoDataset('../tests/data/media/video_image_folder/', 'frame_{:05d}.jpg')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the dataset's will load an example and convert it into a tensor. If you wish to perform data augmentation you should pass a `transform` to the constructor which will receive an iterator of `PIL.Image.Image` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is layed out in the `CTHW` format, that is `(channels, time, height, width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VideoFolderDataset\n",
    "\n",
    "This is the dataset class you're looking for if you've got a folder of video files, each representing an example in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VideoFolderDataset` expects a directory of video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../tests/data/media\n",
    "echo \"Top level folder contents: \"\n",
    "ls -l video_folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFolderDataset('../tests/data/media/video_folder/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the dataset's will load an example and convert it into a tensor. If you wish to perform data augmentation you should pass a `transform` to the constructor which will receive an iterator of `PIL.Image.Image` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GulpVideoDataset\n",
    "\n",
    "If you've [gulped](https://github.com/TwentyBN/GulpIO#gulp-a-dataset) your data, then you'll want to use this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GulpVideoDataset` uses the [GulpIO](https://github.com/TwentyBN/GulpIO) format. You have to 'gulp' your files and then point it toward the root directory containing the `*.gulp` and `*.gmeta` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../tests/data/media\n",
    "echo \"Top level folder contents: \"\n",
    "ls -l gulp_output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've stored your example labels in the metadata for each segment, you can access them by passing `label_field` with the name of the field in the `gmeta` JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GulpVideoDataset('../tests/data/media/gulp_output', label_field='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset has been constructed with a `LabelSet` it'll return both the frames and labels for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = dataset[0]\n",
    "print(\"Label: \", label)\n",
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels \n",
    "\n",
    "Typically you're going to want to get the label for an example when you load it; this is the job of the `LabelSet`. By decoupling the reading of video data and storage of metadata we facilitate a flexible model where you can pick how to store your labels and how you store you video data so you can mix and match. \n",
    "\n",
    "A `LabelSet` is class that provides a `__getitem__` object that when given the filename of a video folder, or file, or video id returns the corresponding label. All `VideoDataset` classes support the `label_set` kwarg in their constructors, so you can pass a label set to any subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DummyLabelSet` will return the same label regardless of what it is passed, it's useful for testing/using code that demands a label and you just have to fake it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = DummyLabelSet(label=5)\n",
    "label_set['video1'], label_set['any_video']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a dataset is given a label set, it will return both the frames and labels upon loading a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolderVideoDataset(\n",
    "    '../tests/data/media/video_image_folder/', 'frame_{:05d}.jpg',\n",
    "    label_set=label_set\n",
    ")\n",
    "\n",
    "frames, label = dataset[0]\n",
    "print(\"Video shape:\\t\", frames.shape)\n",
    "print(\"Label:\\t\\t\", label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the storage format of labels is very dataset dependent so we provide a `LambdaLabelSet` class that wraps a user provided function that will return a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels_df = pd.DataFrame({\n",
    "    'video':['video0', 'video1', 'video2'],\n",
    "    'label': [1, 2, 3]\n",
    "}).set_index('video')\n",
    "label_set = LambdaLabelSet(lambda filename: labels_df.loc[filename]['label'])\n",
    "\n",
    "label_set['video0'], label_set['video1'], label_set['video2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, we've always loaded all the frames from a video into memory. Loading frames is typically one of the most expensive operations in a video ML pipeline, so we would like to minimize this cost as much as possible and only load the frames we want.\n",
    "\n",
    "Currently there are two dominent classes of frame sampling methods in video machine learning: dense sampling, where we sample a contiguous sequence of frames, sparse sampling, where we sample frames far apart from each other.\n",
    "\n",
    "We provide a variety of samplers supporting these sampling strategies.\n",
    "\n",
    "The purpose of a frame sampler is to generate a set of frame indices given the length of a video in frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FullVideoSampler` is the default sampler used in all dataset classes, it generates a `slice` object covering the entire video clip. If we had a video 20 frames long, we get a slice of starting from frame 0 ending at frame 19 with step size 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = FullVideoSampler()\n",
    "sampler.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also control downsample the video by setting the `frame_step` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = FullVideoSampler(frame_step=2)\n",
    "sampler.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame samplers can return 3 types of indices, this is so that the video loaders can load in the most optimal fashion. It's helpful to convert these representations into a list of ints to ease the cognitive load in trying to understand which frames are being sampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvideo.samplers import frame_idx_to_list\n",
    "\n",
    "sampler = FullVideoSampler(frame_step=2)\n",
    "frame_idx_to_list(sampler.sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit easier to see exactly what `slice(0, 20, 2)` represents in terms of frame indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many methods utilize a fixed duration of clip sampled from a larger video. The `ClipSampler` implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ClipSampler(clip_length=10, frame_step=1)\n",
    "frame_idx_to_list(sampler.sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ClipSampler(clip_length=10, frame_step=2)\n",
    "frame_idx_to_list(sampler.sample(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse sampling methods sample very few frames far apart. The `TemporalSegmentSampler` samples frames in the way described in the [TSN paper](https://arxiv.org/abs/1608.00859)\n",
    "\n",
    "Basically a video is split into `n` segments, and then a snippet of video `l` frames long is sampled from each of these segments with a random offset during training, and centred within the segment during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = TemporalSegmentSampler(segment_count=3, snippet_length=2)\n",
    "frame_idx_to_list(train_sampler.sample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = TemporalSegmentSampler(segment_count=3, snippet_length=2, test=True)\n",
    "frame_idx_to_list(test_sampler.sample(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
