{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchvideo Transforms demo\n",
    "\n",
    "This notebook demonstrates the video transforms present in `torchvideo`.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Set up](#Set-up)\n",
    "  1. [Imports](#Imports)\n",
    "  2. [Downloading media](#Downloading-media)\n",
    "2. [PIL Video transforms](#PIL-Video-transforms)\n",
    "  1. [CenterCropVideo](#CenterCropVideo)\n",
    "  2. [RandomCropVideo](#RandomCropVideo)\n",
    "  3. [RandomHorizontalFlipVideo](#RandomHorizontalFlipVideo)\n",
    "  4. [MultiScaleCropVideo](#MultiScaleCropVideo)\n",
    "  5. [RandomResizedCropVideo](#RandomResizedCropVideo)\n",
    "  6. [TimeApply](#TimeApply)\n",
    "3. [Torch.*Tensor transforms](#Torch.*Tensor-transforms)\n",
    "  1. [NormalizeVideo](#NormalizeVideo)\n",
    "  2. [TimeToChannel](#TimeToChannel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the library path to sys.path so we can import torchvideo\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvideo.transforms import *\n",
    "from torchvideo.datasets import *\n",
    "from torchvideo.samplers import *\n",
    "from torchvideo.tools import show_video\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download a test video and prepare the some toy datasets. We'll reuse the media used to test `torchvideo`. The `gen_test_media` script will download a short clip of [Big Buck Bunny](https://peach.blender.org/) and create datasets suitable for use with all `VideoDataset` classes:\n",
    "\n",
    "- An [`ImageFolderVideoDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#imagefoldervideodataset) where each example is comprised of a set of frames stored as images on disk.\n",
    "- A [`VideoFolderDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#videofolderdataset) where each example is stored as a video file.\n",
    "- A [`GulpVideoDataset`](https://torchvideo.readthedocs.io/en/latest/datasets.html#gulpvideodataset) where frames are stored in a simple binary format of concatenated JPEGs (see the [GulpIO](https://github.com/TwentyBN/GulpIO) README for more info on this format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download the test media\n",
    "if [[ ! -f ../tests/data/media/big_buck_bunny_360p_5mb.mp4 ]]; then\n",
    "  cd ../tests/data/media \n",
    "  ./gen_test_media.sh > /dev/null 2>&1\n",
    "  cd -\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can play around with `torchvideo`'s transform classes. But first we'll need to get a video to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFolderDataset('../tests/data/media/video_folder/')\n",
    "video = dataset[0]\n",
    "type(video), video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `VideoDataset` classes process the data into a CTHW format suitable for training a network. We'll convert this back to a representation that we can display in this notebook using `show_video`, a little function included in `torchvideo` for playing around with transforms.\n",
    "\n",
    "Here's our original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the frames a list of PIL.Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VideoFolderDataset will pass an iterator of PIL Images to the transform\n",
    "# so we need to collect the frames from this iterator into a list.\n",
    "dataset.transform = CollectFrames()\n",
    "video = dataset[0]\n",
    "type(video), len(video), type(video[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with the transforms, we'll define a few helper functions to sample the video multiple as many transforms have a random element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tensor_video2clip(tensor_video, fps=24):\n",
    "    # CTHW -> THWC\n",
    "    vid = tensor_video.mul_(255).to(torch.uint8).permute(1, 2, 3, 0).cpu().numpy()\n",
    "    return ImageSequenceClip(list(vid), fps=fps)\n",
    "\n",
    "\n",
    "def flat2grid(seq, n_cols):\n",
    "    n_rows = int(np.ceil(len(seq) / n_cols))\n",
    "    grid = []\n",
    "    for irow in range(n_rows):\n",
    "        row = []\n",
    "        grid.append(row)\n",
    "        for icol in range(n_cols):\n",
    "            i = irow * n_cols + icol\n",
    "            if i == len(seq):\n",
    "                break\n",
    "            row.append(seq[i])\n",
    "    return grid\n",
    "    \n",
    "\n",
    "def demo_transform(transform, n_samples=1, tile_width=3, fps=24):\n",
    "    if n_samples > tile_width and not (n_samples / tile_width).is_integer():\n",
    "        raise ValueError(\"Expected tile_width to divide n_samples perfectly.\")\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        samples.append(tensor_video2clip(transform(video), fps=fps))\n",
    "    samples = flat2grid(samples, tile_width)\n",
    "        \n",
    "    return clips_array(samples).ipython_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now we can play with transforms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PIL Video transforms](https://torchvideo.readthedocs.io/en/latest/transforms.html#transforms-on-pil-videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CenterCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#centercropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    CenterCropVideo((200, 400)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomcropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    RandomCropVideo((150, 300)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((100, 200)),\n",
    "    RandomCropVideo((150, 300), pad_if_needed=True),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((130, 280)),\n",
    "    RandomCropVideo((150, 300), padding=(20, 20, 0, 0)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomHorizontalFlipVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomhorizontalflipvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((184, 320)),\n",
    "    RandomHorizontalFlipVideo(),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=4, tile_width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MultiScaleCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#multiscalecropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((184, 320)),\n",
    "    MultiScaleCropVideo((100, 200), max_distortion=2),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomResizedCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomresizedcropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    RandomResizedCropVideo((100, 200)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TimeApply](https://torchvideo.readthedocs.io/en/latest/transforms.html#timeapply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    TimeApply(Grayscale()),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torch.*Tensor transforms](https://torchvideo.readthedocs.io/en/latest/transforms.html#transforms-on-torch-tensor-videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NormalizeVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#normalizevideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "    NormalizeVideo(128, 100)\n",
    "])\n",
    "x = dataset[0].cpu().numpy()\n",
    "x.shape, x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TimeToChannel](https://torchvideo.readthedocs.io/en/latest/transforms.html#timetochannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "    TimeToChannel()\n",
    "])\n",
    "dataset[0].cpu().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
