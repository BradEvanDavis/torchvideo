{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform demos\n",
    "\n",
    "This notebook demonstrates the transforms provided by torchvideo.\n",
    "\n",
    "First we need to do some set up, and load some data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "3.7.2 (default, Dec 29 2018, 21:15:15) \n",
      "[GCC 8.2.1 20181127]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvideo.transforms import *\n",
    "from torchvideo.datasets import ImageFolderVideoDataset\n",
    "from torchvideo.datasets.vis import show_video\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale\n",
    "\n",
    "# original video size = 640x368\n",
    "\n",
    "transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolderVideoDataset('../tests/data/media/video_image_folder/', 'frame_{:05d}.jpg', transform=Compose([]))\n",
    "video = list(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(video), type(video[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "])\n",
    "show_video(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tensor_video2clip(tensor_video, fps=24):\n",
    "    # CTHW -> THWC\n",
    "    vid = tensor_video.mul_(255).to(torch.uint8).permute(1, 2, 3, 0).cpu().numpy()\n",
    "    return ImageSequenceClip(list(vid), fps=fps)\n",
    "\n",
    "\n",
    "def flat2grid(seq, n_cols):\n",
    "    n_rows = int(np.ceil(len(seq) / n_cols))\n",
    "    grid = []\n",
    "    for irow in range(n_rows):\n",
    "        row = []\n",
    "        grid.append(row)\n",
    "        for icol in range(n_cols):\n",
    "            i = irow * n_cols + icol\n",
    "            if i == len(seq):\n",
    "                break\n",
    "            row.append(seq[i])\n",
    "    return grid\n",
    "    \n",
    "\n",
    "def demo_transform(transform, n_samples=1, tile_width=3, fps=24):\n",
    "    if n_samples > tile_width and not (n_samples / tile_width).is_integer():\n",
    "        raise ValueError(\"Expected tile_width to divide n_samples perfectly.\")\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        samples.append(tensor_video2clip(transform(video), fps=fps))\n",
    "    samples = flat2grid(samples, tile_width)\n",
    "        \n",
    "    return clips_array(samples).ipython_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now we can play with transforms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PIL Video transforms](https://torchvideo.readthedocs.io/en/latest/transforms.html#transforms-on-pil-videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CenterCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#centercropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    CenterCropVideo((200, 400)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomcropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    RandomCropVideo((150, 300)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((100, 200)),\n",
    "    RandomCropVideo((150, 300), pad_if_needed=True),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((130, 280)),\n",
    "    RandomCropVideo((150, 300), padding=(20, 20, 0, 0)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomHorizontalFlipVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomhorizontalflipvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((184, 320)),\n",
    "    RandomHorizontalFlipVideo(),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor()\n",
    "]), n_samples=4, tile_width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MultiScaleCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#multiscalecropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    ResizeVideo((184, 320)),\n",
    "    MultiScaleCropVideo((100, 200), max_distortion=2),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RandomResizedCropVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#randomresizedcropvideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    RandomResizedCropVideo((100, 200)),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TimeApply](https://torchvideo.readthedocs.io/en/latest/transforms.html#timeapply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transform(Compose([\n",
    "    TimeApply(Grayscale()),\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "]), n_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torch.*Tensor transforms](https://torchvideo.readthedocs.io/en/latest/transforms.html#transforms-on-torch-tensor-videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NormalizeVideo](https://torchvideo.readthedocs.io/en/latest/transforms.html#normalizevideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "    NormalizeVideo(128, 100)\n",
    "])\n",
    "x = dataset[0].cpu().numpy()\n",
    "x.shape, x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    Stack(),\n",
    "    ToTorchFormatTensor(),\n",
    "    GroupNormalize([128], [100])\n",
    "])\n",
    "x = dataset[0].cpu().numpy()\n",
    "x.shape, x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.flip(dataset[0], (0,)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TimeToChannel](https://torchvideo.readthedocs.io/en/latest/transforms.html#timetochannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = Compose([\n",
    "    CollectFrames(),\n",
    "    PILVideoToTensor(),\n",
    "    TimeToChannel()\n",
    "])\n",
    "dataset[0].cpu().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
